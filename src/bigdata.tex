\section{Big Data} It can do so in various ways
(location- or spatial awareness, etc).
Data is aquired by sensor, simulations or feeds. Can we filter or compress the feed?
Can we trust the sensors (are they faulty?)
Information extraction: seeing structure in data, amending it where necesessary.
Aggregation: heterogenous sources needs to integrate.
Modeling. Note that the data can be noisy. Still might be worthy to visualize and interpret.

Challenges: heterogenity, inconsistency, incompleteness, scale, timeliness\dots
Privacy is also becoming an increasingly important concern now that we get data from just about everything.

A trend in NoSQL and BDMS is horizontal scalibility over vertica. That is, we prefer having many 
servers over one powerful one. Across these shards of data we do horizontal partitioning: one file is 
split across nodes.

Since NoSQL is inherently less structured, its queries are simpler, often a minima to provide CRUD operations. 

There are four types of databanks for NoSQL DB's:
\begin{enumerate}
    \item \textbf{Document-based:} store a document and an ID, query by the ID.\@
        \newline Example: \nameref{sec:mongodb}.
    \item \textbf{key-value:} allow custom keys and custom values (e.g.\@ one document can have multiple indexes for multiple values).
        \newline Example: \nameref{sec:cassandra}, \nameref{sec:voldemort}.
    \item \textbf{column-based:} instead of just having documents, you structure each record \textit{by their columns}. That means that you have one row for each column value $X$, e.g.\@ URL, then another row for $Y$, e.g.\@ timestamp. The neat thing about this is being able to read only the data you need, instead of processing
        entire tuples.
        \newline Example: \nameref{sec:HBase}
    \item \textbf{Graph-based:} cool stuff. One cool advantage is the simple queries for complex operations.
        \newline Example: \nameref{sec:neo4j}
\end{enumerate}

\subsection{Definitions}

\begin{definition}[Log-structured Merge Tree]\label{def:LSM}
    (LSM) A data-structure optimized for indexed access with a high insert volume.
    It typically stores a lot of data in key-value form in-memory, and flushes out
    when exceeding a memory threshold.
\end{definition}

\begin{definition}[Write-ahead log]\label{def:WAL}
    WAL:\@ write action before doing them. Important for \nameref{def:acid} properties.
\end{definition}

\begin{definition}[Consistent hashing]\label{def:consistenthashing}
    A hashing scheme that scales well. When you add keys (expand domain), you don't need to
    remap the majority of the keys as you would have to in normal hashing.
    This can be thought of as extending a simple hashing algorithm $h(x) = \mod k$.
    When we expand the domain of the function, it will simply result in a lower hashvalues that can
    be used directly without changing other indices.
\end{definition}

\begin{definition}[ACID]\label{def:acid}
    \begin{description}
        \item[Atomicity:] An action will either completely fail or completely succeed, nothing  in between.
        \item[Consistency:] An action will either completely fail or completely succeed, nothing  in between.
        \item[Isolation:] Executing multiple actions simultaneously yields the same result as if all the actions
            were executed serially (that is, no messing up because of pararell execution).
        \item[Durability:] If an action is committed, it will remain in the system even if there are
            power losses, crashes or errors.
    \end{description}
\end{definition}

\begin{definition}[Uber task]\label{def:ubertask}
    A term used in MapReduce to denote a job so small that pararellizing it
    will not cause a benefit, thus leaving the job to be resolved in the same JVM 
    as the master.
\end{definition}

\begin{definition}[BASE]\label{def:base}
    An alternative way of analyzing DBMS.\@ It is less strict than \nameref{def:acid}.
    \begin{description}
        \item[Basic Availibility] database should work most of the time
        \item[Soft-state] replicas don't always have to be consistent
        \item[Eventual consistency] An update doesn't have to be seen by all peers right away
    \end{description}
\end{definition}

\begin{definition}[CAP theorem]\label{def:captheorem}
    Working with Big Data involves dealing with inputs so large that conventional methods do not work.
    This also involves that we have to compromise, not opting for ``optimal'' solutions like RDMS.\
    We have 
    \begin{description}
        \item[Consistency:] Whether or not copies of data are the same across all nodes.
        For example, having a server in London with data X and another server in USA with data X', where X' is intended to be equal to X, but it is not.
        \item[Availibility:] Whether or not we can guarantee success or failure (alternatively: every request returns a non-error response).
        \item[Partition tolerance]:  If a node goes down, will the system continue to operate?
    \end{description}

    The CAP theorem states that any big-data system can only acheive two out of tree letters (CA, CP or AP).
\end{definition}

\begin{proof}
Assume a system has two nodes: A and B. A and B cannot communicate.
We write data X to node A and B. Then, we write $X'$ to node A. Following that we want to read $X$ from node B.
If node A and B do not talk together, we will not achieve consistency (node B doesn't know X is updated in A).
This also means we struggle with availibility: $X'$ has not been written to both nodes.
If we do let node A and B talk together, then node B depends on A, so we are not parition tolerant.
Hence, achieving all three letters is not possible in this case.
\end{proof}

\begin{definition}[Five V's of Big Data]\label{def:fiveV}
    \begin{description}
        \item[Volume:] How much data do we have?
        \item[Veracity:] Can we trust the data we have?
        \item[Variety:] What types of data do we read?
        \item[Value:] Is it really worth while to do big data solutions?
        \item[Velocity:] How fast is data coming in?
    \end{description}
\end{definition}

\begin{definition}[At-least-once ingestion]\label{def:atleastonce}
    If any message is lost, retransmit it. Thus, we can guarantee that a message will be received.
    There is a possibility of duplicates.
\end{definition}

\begin{definition}[At-most-once ingestion]\label{def:atmostonce}
    Every message will be sent, but at most one time.
    There is a possibility of duplicates.
\end{definition}

\begin{definition}[Fault Tolerance]
    In Big Data, there are two types of failure that can occur
    \begin{description}
        \item[Soft:] unexpected errors, such as null values or code error
        \item[Hard:] loss of nodes, etc.
    \end{description}
\end{definition}

\begin{definition}[Exactly once ingestion]\label{def:exactlyonce}
    Every message is delivered once. No duplicates. Hard to implement (you need to retain data for a longer amount of time and have synchronization protocols).
\end{definition}

\begin{definition}[Equi-depth histogram]\label{def:equidepthhistogram}
Take query: 
\begin{verbatim} SELECT * FROM Person WHERE AGE > 24 \end{verbatim}.
Here, we can optimize the query by using an equidepth histogram:
instead of preparing to return each row, we first process the $AGE$ column and count 
how many occurences we get. Then, we estimate parallelism, return size, etc, and return queries.
If $AGE$ is indexed, we could make estimates like  this in $O(1)$.
\end{definition}

\begin{definition}[Equi-width histogram]
    Similar to \nameref{def:equidepthhistogram}, except we fix bucket sizes.
    For example, for an age query we don't return the number of people above a threshold age,
    instead we bucketize, e.g. everyone under 24 in one bucket, everyone over 24 in another,
    or splitting per 5 years, etc.
\end{definition}

\subsection{Q and A's}

Q:\@ how can a join be affected by whether or not we sort the output?
\newline A:\@ When we do JOINS, we lookup keys from table X and Y. We put together keys from e.g. Y into table X. Therefore, table X needs to be either sorted or use some form of hashing scheme so that we can efficiently find the key in X that matches the key in Y.

If data is assumed ordered, we save a great deal of time. All we need to do is iterate over table Y, and allocate each key to its corresponding bucket from table X. This will be joined data. We will not shuffle table X around much.  

Q:\@ discuss joins in relation with parallelism?
\newline A:\@ If we have multiple datanodes availible, we can do mapping 
parallelized. There is more overhead since we need to merge partitions afterwards,
but for large inputs the tradeoff is still positive.

Q:\@ describe diffs in traditional DBMS and data streams.
\newline A:\@ There are some obvious differences in terms of technology. Streaming technology has key uses 
in fields where automated decisions are necessary. It can be used for controlling prices, 
shutting down faulting hardware or giving an early warning to monitoring analysts.
They will not directly  support saving any form of state. Doing so introduces a significant overhead,
and it may be worth-while to consider writing to disk first. 
An interesting side-note is that some RDBMS like PostgreSQL offer \nameref{def:atmostonce} analysis, quick processing and horizontal scaling. For applications that need both streaming and storage, this can
be a good alternative.

Q:\@ Should file partitions always have the same size?
A:\@
It is interesting to note that if a partition is sufficiently small, it can fit into memory.
Since memory can dynamic, varying partition size can be desirable to fit as much in memory as possible.
Otherwise, it is usually preferable to choose a fixed partition size so that sharding and 
other operations are easier, and to prevent stalls induced by having some nodes finish up
before others due to a smaller partition sizes.
\newline A:\@ ah.

Q:\@ Explain a sliding window in streaming systems
\newline A:\@ Iterating over windows at a time. Entries are  iterated mutiple times since they
will occur in multiple slides. Old data falls outside of the window because it is no longer relevant.

\subsection{Misc}
\subsubsection{Yet Another Resource Negotiator}\label{sec:YARN}
YARN has a resource manager per cluster. Inside each each cluster there are multiple
node managers. The node managers manage containers, which is the equivalent of 
a process.

Clients can make resource requests, asking for locality of nodes or a given amount of memory.
This is stated up front, but can also be changed dynamically.

Allocation can be per-user, per-session or interchanged between users.
Per-user is often used in MapReduce, where the client is given a set of nodes
that are used once, and then released.
A per-session is used for iterative jobs (common in e.g. \nameref{sec:spark}), so that
after finishing the first stage of a job, the same nodes are allocated for the second iteration as well.
The third is more appropriate if you for example have many jobs from different companies or applications
and want to fix or avoid allocation errors (giving each company a set of nodes, for example).
Also common for long-term jobs that go for several days and/or months.

Comparing YARN to MapReduce 1, we see a bigger separation between entities. There 
is no single jobtracker: its job is separated into resource manager, application masters
and timeline servers. The improved structure allows them to scale more, provide higher
availibility (eliminating/reducing single points of failure), utilize resources more effectively,
and do multitenancy. The latter means that you can have multiple jobs on the same clusters. Furhtermore,
it is no longer just MapReduce jobs that are executed, but other frameworks like Spark and Storm enter the scene.

YARN has three schedulers. There is FIFO, Capacity and Fair schedulers.
FIFO is simple to configure but doesn't scale well with large clusters.
Capacity retains simplicity: it provides each user with a small, reserverd amount of resources.
This pool of resources is seen as a small and private FIFO queue. If availible, the job will be upgraded
with more resources to do its computations. Capacity is a good scheduling algorithm for multi-company uses
of a WSC datacenter.  Fair scheduler dynamically allocates resources to jobs.
When a single job is on the cluster, it receives full resource utilization. When another job comes in,
it receives half of the resources, and so on.




\subsection{File Systems}
\subsubsection{Google File System}\label{sec:GFS}
One of the first file systems to scale with big data. The main idea is to replicate data
exensively to provide high availibility and reduce transmission costs from having to send data on-demand.
Data is most often ``written once, updated seldom'' which allows writes to be inefficient (unlike some other RDBMS). Aso, whenever you modify an entry, you typically prefer appending modifications rather than re-writing it.

Another interesting feature of GFS is that it typically runs on commodity hardware rather than
specialized computers. This induces a higher risk of hardware failure, which needs to be dealt with.
One exapmle solution is to implement frequent ``heartbeats'' from commodity computers to a master node.

GFS is optimized for MapReduce; the paritioning scheme makes map-reduce jobs simple.

For security reasons, GFS also obfuscates its data with randomization algorithms.

\subsubsection{HDFS}\label{sec:HDFS}
Very similar to \nameref{sec:GFS}. It was intended to be open-source alternative to GFS.\@
It can now support more nodes and has different security/permissions model, often from POSIX.
Users of \nameref{sec:HBase} often store info in HDFS.
Each block has two files, one for metadata, and one for the data itself.

Namenodes contain info about where datanodes are. This lookup table in-memory.
Datanodes get to know namenodes by handshake. Namenodes never contact datanodes, they only reply
to heartbeats.
Critical components for the namenode are the journal and checkpoint data.
These are stored redundantly. Also, the namenode itself has redundant entities like the backup nodes.

An interesting concept is that data is considered immutable (appends are allowed). Thus, deletions are
done by inserting a \textit{tombstone marker} to indicate that the data is dead.

\subsection{Big Data Management Systems (BDMS)}
\subsubsection{AsterixDB}\label{sec:asterixdb}
Like all other BDMS, AsterixDB comes with its own superflous words to describe
normal things. The top-level node is called a \textit{dataverse}. Other papers describe
dataverses as databases. Unlike other BDMS, AsterixDB has a strict type policy,
which means that it will do type-checking on that data inserted into a dataverse.
It is possible, however, to make a loose definition of type, thus allowing random data
to enter the dataverse (naturally called having ``wiggle room'').

Instead of using JSON, AsterixDB uses something called ADM: an
extension of JSON. Keep in mind that it is still possible to have strict
requirements for the structure of JSON data. 

All data-records have a key. Keys can be composite or optimized for fuzzy string search.
Each dataset has a $B^{+}$-tree with key-value pairs.

AsterixDB can read non-indexed data, e.g.\@ loading a CSV-file. This will be mounted read-only.
This helps them read data-feeds just as well as data from e.g.\@ HDFS. However, AsterixDB does not operate
directly on the stream directly, it requires structuring data before queries are executed.
The neat benefit from this is that AsterixDB hides complexity; quering a feed is exactly
the same as quering a stored dataset from a developer point-of-view.

Continuing in the BDMS-framework fashion, AsterixDB defines its own query language.
AQL is similar to XQuery (for XML), except they claim AQL is simpler. AQL is compiled down to algebricks
which can be optimized. The output is a (often parallel) Hyracks job.

Keys are stored as 2-way \nameref{def:LSM}, which opens up for rapid insertions 
(aka dealing with high ingestion).

AsterixDB supports transactions and is thus ACID-compliant.



\subsubsection{Cassandra}\label{sec:cassandra}
Developed at Facebook for inbox search.
High throughput, but at the price of high write and read latency.
Nodes are independent, but connected to all other nodes.

Key-value ish, however, the value is highly structured (unlike other big data
management systems).  Very scalable, as mostBDMS are.
Uses \nameref{def:consistenthashing} to do so, and has different modes for replication
such as ``rack aware'' and ``datacenter aware'' schemes.

\subsubsection{HBase}\label{sec:HBase}
While \nameref{sec:GFS} and MapReduce was great, no one in Big Data
had real-time access to data. Enter HBase. Column-oriented file storage.
HBase provides \textit{strict consistency}, i.e.\@ no reordering of instructions.

Uses ZooKeeper to track nodes. There should be one master alive at all times.
If an application loses nodes (misses heartbeats), associated nodes are deleted.
Master nodes does load balancing.  Split are done by dividing in two.

No complex queries are supported. Only calls to API. 

HBase is good for scientific applications and analysis over all data.

\subsubsection{Neo4j}\label{sec:neo4j}
Intuitively cool, Neo4j is a graph database. It is \nameref{def:acid} compliant (notably, unlike the systems discussed above it has support for transactions).
Queries are simple, even for complex joins. Unlike EER diagrams (which Neo4j might be
similar to), vertices may have no type and relationships are directed.


\subsubsection{MongoDB}\label{sec:mongodb}
\epigraph{MongoDB (from hu\textbf{mongo}us)}{Wikipedia}

MongoDB stores data in BSON, i.e.\@ binary JSON. It can index its data in multiple ways:
\begin{description}
    \item[Array] Each entry of an array can be indexed
    \item[Compound] Indexes built on multiple fields such as ``Name  + Age''
    \item[Geospation] Coordinate points that refer to a location, etc
    \item[Partial] Indexes that reflect data. For example get all customers that did something the last 24 hours
    \item[Sparse] Index only documents that has a certain field (different from partial where the value of the field is used)
    \item[TTL] Indices that expire after a given amount of time
    \item[Text Search] Index a text and provide a key that indicates the text's relevance in accordance with query
    \item[Unique] Normal ones!
\end{description}

Naturally, given the amount of indexes that can be used, there are also many queries.
Some examples are geospatial queries, range queries and MapReduce. The latter is often
executed as a javascript code query, a nifty feature of MongoDB.

One other nifty feature of MongoDB is autosharding. As long as you provide clusters
and computers, MongoDB is capable of balancing the load for you. It can do so in various ways
(location- or spatial awareness, etc).

A downside of MongoDB (andBDMS in general) is that joins
aren't supported natively. 

With replicas, MongoDB always selectes a primary as the main provider for data.
Only the primary is written to, then information is propagated.

MongoDB compresses its data.

MongoDB is ACID compliant at a document level. However, for updating all of its replicas, 
there is a possibility (albeit small) that an error will occur. A use case that is particulary relevant
is customer purchase: first remove item from the inventory, then add that item to a customer.
Now you need to do two actions in one go. Normal RDBMS will handle this with transactions,
however, in MongoDB there is a chance that you are able to remove the item from inventory, but not add
it to the customer. If this happens, you are not \nameref{def:acid} at a higher level, although you have \nameref{def:acid} at document level. The property failing is (like Vodlemort) isolation.
In regards to \nameref{def:captheorem}, MongoDB offers eventual consistency

\subsubsection{Spark}\label{sec:spark}
Arguably better for analytics than queries. This is because it primarily works on 
\nameref{sec:HDFS}.

One of Spark's genius concepts is to create small subsets of data seen as read-only objects with lazy evaluation.
This means we can track \textit{lineage} and restore data upon failure. Also, their \textit{resilient distributed datasets} (RDD) are great for pararell and iterative executions of data.
This is seen for example by \textit{persisting} RDDs, saving RDD-state and re-using them in several computations.
Persistance can be done in-memory, to disk or both. Persisted RDD are immutable to ensure consistency.
This is not a feature seen in e.g.\@ distributed shared memory (DSM). The advantage for the latter, however,
is that modifying data is easier.



\subsubsection{Storm}\label{sec:storm}
Developed by Twitter, storm is real-time and fault-tolerant API/framework.

To analyze storm we talk about data as directed edges in a graph, moving between vertices
that represent computations. Data is in the form of a stream of tuples.
Each graph is for whatever reason called a topology. In each vertex there are
two disjoint sets: a spout and a bolt. The spout receives the data, and the bolts
process them. Each bolt may direct its data back to a spout if it wants to (achieving cyclic iterations).

Storm uses the concept of master and worker-nodes. The master node is responsible for delegation.
A worker node can have multiple jobs, but only one topology at a time. Worker nodes have 
monitors called supervisors that talk to the master node.
Every relationship has synchronizations: the worker sends heartbeats to its supervisor, the supervisor
synchronizes its master,  and the supervisor synchronizes its topology.

Storm has a summingbird that can optimize queries for you.

Storm has \nameref{def:atleastonce} and \nameref{def:atmostonce}. The former is ensured
by adding ``acker'' bolts to tuples outputted from a spout. This is done by appending
an ackerbolt to the topology and a 64-bit field-value to each tuple. If the ack-value
isn't received after a given amount of time, it is replayed. The latter is
ensured by default if \nameref{def:atleastonce} is disabled.




\subsubsection{Voldemort}\label{sec:voldemort}
\epigraph{It is basically just a big, distributed, persistent, fault-tolerant hash table}{Internet}

Used for LinkedIn and their data. It provides a data-store with key-value
pairs. This is simple to deal with, but eliminates possibilities for comlex
queries, foreign keys, triggers, etc.  Values can be either JSON or whatever format the user desires.
The hashmaps can also be in-memory, meaning they're fast.

Alike many otherBDMS, Voldemort focuses more on AP than
consistency (see \nameref{def:captheorem}). The level of consistency can,
however, be adjusted. Often, however, it is left to the client, who receives multiple
copies of data if there is a risk of concurrent writes having corrupted a value.
What is interesting is that they do not satisfy all
\nameref{def:acid} properties; data isolation is not ensured. 

Voldemort is different than e.g. HBase in that they focus on having a large
amount of writes in their systems.  In Hbase this is not prioritized,
henceforth it is slow. Also, with the use of hashmaps, their data is inherently
more unstructured.

Voldemort has low latency, but not the best throughput (like Cassandra).

\subsection{Methods / Algorithms}
\subsubsection{PageRank}
Initially, give each node a rank. For example all nodes start with rank $\frac{1}{N}$, where N is the number of pages.
Now, iterate over all the nodes, giving each node $x$ rank in iteration $t$:
\begin{equation}\label{eq:pagerank}
    PR_{t}(x) = \frac{1 - \lambda}{N} + \lambda\sum\limits_{{y \rightarrow x}{}}{\frac{PR_{t}(y)}{out(y)}}
\end{equation}
The LHS of \nameref{eq:pagerank} is the probability of being at this given node. The RHS is the probability of being at any given node, multiplied by its pagerank \textit{for the current iteration}, for all connecting vertices. The \textit{out} for each node is constant, and refers to how many edges there are pointing out from node $y$. The intuition is that a node may have a high pagerank (which makes $x$ more likely), however, if $y$ has many nodes, it reduces the chances for $x$ (which is why we divide PR over out).

Note that pageranks should, in the end, sum up to 1 (or 100\%). 


\subsection{MapReduce 2.0}
Client asks resource-manager for resources.
Resource-manager verifies everything is OK, and sends back info about an availible container. Client is happy, and uploads JAR.
If the first container (appmaster) realizes this is not an \nameref{def:ubertask}, it will ask resource-manager for more buddies. It will prefer local nodes, but the resource-manager can overrule priorities if there are conflicts. Resource-mananger complies.
Now the job is running. When finished, application managers report that we are done, 
and releases its resources. As an extra side-note, it might be worth noting that the
reduce-part can begin before all maps are finished (to save time).
Failure results in re-trying the code on a different container $n$ times before giving up. Failure can either be tolerated up to $x$ percent or result in halted execution.
AppMaster can fail and be restarted from its log files. This is done by the resource-manager. Since the resource-manager plays such a critical role, its function
is often duplicated across several nodes to prevent having a single point-of-failure.
All of the information processed by resource-managers are also stored in high-availibility systems like HDFS or using ZooKeeper.

\subsubsection{Shuffle}
When a Map has been performed, we need to output data to reducers. Before this happens,
we need to \textit{shuffle}. This is sorting the data by which reducer it will end up with. This is done by having a small, circular buffer being filled up. When it reaches
a given size it is spilled to disk. The spill files are re-iterated in the end to create
a bigger end file. When the \textit{shuffle} is finished the \textit{copy phase} begins: take data from mapper to reducer. Upon arriving by the reducer, the data is sorted again.

A nice thing to keep in mind is to leave the shuffle phase to have as much memory as possible. This will typically give the best results for MapReduce jobs.

In mapreduce 1.0, map and reduce part always had the same resources (not dynamic). Also, you could only execute mapreduce jobs (not complex shuffles, etc).

\subsubsection{Joins}
A Join is a means for combining fields from two tables by using values common to each.
If one table is small, we often like to distribute it to all nodes such that we 
can quickly lookup all matching entries. If not, we need to be clever.


\begin{definition}[Reduce-side join]\label{def:reducejoin}
    On the map-side of things, we simply sort the data by join-key. E.g. a mapper A
    will give reducer A' all entries with key K. Then, reducer A' performs another sort,
    joining entries from two or more tables on K.
\end{definition}


\begin{definition}[Map-side join]\label{def:mapjoin}
    Similar to \nameref{def:reducejoin}, but we avoid the second pass of sorting.
    A few conditions are necessary to do the join in the first stage of our job.
    \begin{enumerate}
        \item Data is already sorted by join key inside partitions
        \item Input datasets have same number of partitions (difficult if doing joins 
            for $> 2$ tables).
    \end{enumerate}
\end{definition}

\begin{definition}[Side-Data]\label{def:sidedata}
    Read-only data needed by a job to process a dataset.
\end{definition}

\subsection{Comparing Frameworks}
When do we want to choose a given API over the other? It depends.
There are no ``one size-fits-all'' in Big Data. Furthermore, under the constraints of
the \nameref{sec:captheorem}, most BDMS make some sort of compromise. Hence,
the choice of API matters a great deal, as some frameworks are much more optimized
for given problems.

If you need to do a lot of writes or fetches, in general you want to choose from either
\nameref{sec:cassandra} or \nameref{sec:voldemort}. They both have low latencies, they're simple in use. Cassandra has LSM-trees and is henceforth slightly more optimized for writes, however, the LSM-trees also means that \nameref{sec:voldemort}
is slightly better at getting small reads. Cassandra also supports range queries, bloom filters, MapReduce, and other more complex operations. In both of these frameworks, all
nodes are considered to be equal (no concept of master or slave nodes). This
results in difficulty ensuring consistency, since two nodes may easily diverge
without having a master to synchronize them.

Quora has a post where they compare \nameref{sec:hbase} and \nameref{sec:cassandra}. 
Here they say that HBase is also optimized for writes, but it takes consistency much further (with their
strictly consistency policy). Also, HBase can take in semi-structured data, which gives
more room for advanced queries. However, Voldemort is fast, so it can be useful
for write-intensive applications where loosing data is not critical. Another neat thing mentioned is that Cassandra is pluggable, i.e.\@ you can replace the 
back-ends easiler.

\nameref{sec:asterixdb} has the advantage of being a complete BDMS. Thus, unlike 
\nameref{sec:storm}, which has to be ``glued'' together with another file system
like \nameref{sec:mongodb}, AsterixDB provides everything needed.


\subsubsection{Denormalization, Duplication and Intelligen Keys}\label{sec:DDI}
TODO: tougher questions: given condition X and Y, which framework would you use?
    when use streaming vs non-streaming vs hybrid?
TODO: transitioning from RDBMS to BDMS
TODO:  compare yarn to zookeeper? (see hbase for more about zookeeper)
TODO:  how do you sample for counting distinct elements?
    (fjajolet-martin)
