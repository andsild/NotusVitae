\begin{definition}[Log-structured Merge Tree]\label{def:LSM}
    (LSM) A data-structure optimized for indexed access with a high insert volume.
    It typically stores a lot of data in key-value form in-memory, and flushes out
    when exceeding a memory threshold.
\end{definition}

\begin{definition}[Write-ahead log]\label{def:WAL}
    WAL:\@ write action before doing them. Important for \nameref{def:acid} properties.
\end{definition}

\begin{definition}[Consistent hashing]\label{def:consistenthashing}
    A hashing scheme that scales well. When you add keys (expand domain), you don't need to
    remap the majority of the keys as you would have to in normal hashing.
    This can be thought of as extending a simple hashing algorithm $h(x) = \mod k$.
    When we expand the domain of the function, it will simply result in a lower hashvalues that can
    be used directly without changing other indices.
\end{definition}

\begin{definition}[ACID]\label{def:acid}
    \begin{description}
        \item[Atomicity:] An action will either completely fail or completely succeed, nothing  in between.
        \item[Consistency:] An action will either completely fail or completely succeed, nothing  in between.
        \item[Isolation:] Executing multiple actions simultaneously yields the same result as if all the actions
            were executed serially (that is, no messing up because of pararell execution).
        \item[Durability:] If an action is committed, it will remain in the system even if there are
            power losses, crashes or errors.
    \end{description}
\end{definition}

\begin{definition}[Uber task]\label{def:ubertask}
    A term used in MapReduce to denote a job so small that pararellizing it
    will not cause a benefit, thus leaving the job to be resolved in the same JVM 
    as the master.
\end{definition}

\begin{definition}[BASE]\label{def:base}
    An alternative way of analyzing DBMS.\@ It is less strict than \nameref{def:acid}.
    \begin{description}
        \item[Basic Availibility] database should work most of the time
        \item[Soft-state] replicas don't always have to be consistent
        \item[Eventual consistency] An update doesn't have to be seen by all peers right away
    \end{description}
\end{definition}

\begin{definition}[CAP theorem]\label{def:captheorem}
    Working with Big Data involves dealing with inputs so large that conventional methods do not work.
    This also involves that we have to compromise, not opting for ``optimal'' solutions like RDMS.\
    We have 
    \begin{description}
        \item[Consistency:] Whether or not copies of data are the same across all nodes.
        For example, having a server in London with data X and another server in USA with data X', where X' is intended to be equal to X, but it is not.
        \item[Availibility:] Whether or not we can guarantee success or failure (alternatively: every request returns a non-error response).
        \item[Partition tolerance]:  If a node goes down, will the system continue to operate?
    \end{description}

    The CAP theorem states that any big-data system can only acheive two out of tree letters (CA, CP or AP).
\end{definition}

\begin{proof}
Assume a system has two nodes: A and B. A and B cannot communicate.
We write data X to node A and B. Then, we write $X'$ to node A. Following that we want to read $X$ from node B.
If node A and B do not talk together, we will not achieve consistency (node B doesn't know X is updated in A).
This also means we struggle with availibility: $X'$ has not been written to both nodes.
If we do let node A and B talk together, then node B depends on A, so we are not parition tolerant.
Hence, achieving all three letters is not possible in this case.
\end{proof}

\begin{definition}[Five V's of Big Data]\label{def:fiveV}
    \begin{description}
        \item[Volume:] How much data do we have?
        \item[Veracity:] Can we trust the data we have?
        \item[Variety:] What types of data do we read?
        \item[Value:] Is it really worth while to do big data solutions?
        \item[Velocity:] How fast is data coming in?
    \end{description}
\end{definition}

\begin{definition}[At-least-once ingestion]\label{def:atleastonce}
    If any message is lost, retransmit it. Thus, we can guarantee that a message will be received.
    There is a possibility of duplicates.
\end{definition}

\begin{definition}[At-most-once ingestion]\label{def:atmostonce}
    Every message will be sent, but at most one time.
    There is a possibility of duplicates.
\end{definition}

\begin{definition}[Fault Tolerance]
    In Big Data, there are two types of failure that can occur
    \begin{description}
        \item[Soft:] unexpected errors, such as null values or code error
        \item[Hard:] loss of nodes, etc.
    \end{description}
\end{definition}

\begin{definition}[Exactly once ingestion]\label{def:exactlyonce}
    Every message is delivered once. No duplicates. Hard to implement (you need to retain data for a longer amount of time and have synchronization protocols).
\end{definition}

\begin{definition}[Equi-depth histogram]\label{def:equidepthhistogram}
Take query: 
\begin{verbatim} SELECT * FROM Person WHERE AGE > 24 \end{verbatim}.
Here, we can optimize the query by using an equidepth histogram:
instead of preparing to return each row, we first process the $AGE$ column and count 
how many occurences we get. Then, we estimate parallelism, return size, etc, and return queries.
If $AGE$ is indexed, we could make estimates like  this in $O(1)$.
\end{definition}

\begin{definition}[Equi-width histogram]
    Similar to \nameref{def:equidepthhistogram}, except we fix bucket sizes.
    For example, for an age query we don't return the number of people above a threshold age,
    instead we bucketize, e.g. everyone under 24 in one bucket, everyone over 24 in another,
    or splitting per 5 years, etc.
\end{definition}


\begin{definition}[Reduce-side join]\label{def:reducejoin}
    On the map-side of things, we simply sort the data by join-key. E.g. a mapper A
    will give reducer A' all entries with key K. Then, reducer A' performs another sort,
    joining entries from two or more tables on K.
\end{definition}


\begin{definition}[Map-side join]\label{def:mapjoin}
    Similar to \nameref{def:reducejoin}, but we avoid the second pass of sorting.
    A few conditions are necessary to do the join in the first stage of our job.
    \begin{enumerate}
        \item Data is already sorted by join key inside partitions
        \item Input datasets have same number of partitions (difficult if doing joins 
            for $> 2$ tables).
    \end{enumerate}
\end{definition}

\begin{definition}[Side-Data]\label{def:sidedata}
    Read-only data needed by a job to process a dataset.
\end{definition}

